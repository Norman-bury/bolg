---
title: 漫漫炼丹道
published: 2024-04-29
description: 深入探讨深度学习项目的实践操作，处理模型的经验之谈。
tags: [深度学习, 炼丹, 调参]
category: 教程
draft: false
---
# 模型调参与优化实践总结

## 1. BUG FREE
在使用基线代码并进行修改时，确保代码修改无误是关键。常见的问题包括奇怪的loss值，例如不收敛或损失爆炸，往往是由于代码错误导致。因此，验证修改的代码逻辑正确性是必要的。

## 2. Code Pipeline
构建CV模型代码通常包括以下模块：
- **Data (dataset/dataloader)**
- **Model 本身**
- **Optimizer 优化器**
- **Loss functions**
- **Trainer**

### 2.1 Data
- 确保数据集构建正确，并检查输出的数据读取是否正确。

### 2.2 Model
- 使用现有的网络结构，如ResNet或Transformer，并确保任何必要的修改部分是bug free。
- 参数初始化使用PyTorch的内置方法，如kaiming初始化或xavier初始化。
- 确保使用pretrained模型时，模型被正确加载。

### 2.3 Optimizer
- 常用的优化器设置包括使用Adam或AdamW，配合cosine退火策略。
- 学习率设置建议在3.5e-4到5e-5之间，具体可根据实际情况调整。
- 可进行epoch的网格搜索以优化模型性能。

### 2.4 Loss
- 使用标准的loss函数，并确保计算过程中不会出现log(0)等错误。
- 如果出现NaN错误，检查数据输入、loss函数和模型是否有误。

## 3. 学习率调整策略
- 推荐使用warmup后的cosine退减策略，适用于大模型训练。

## 4. 分层学习率策略

## 5. 对抗训练
- 常用方法包括对抗权重扰动（AWP）。

## 6. Stochastic Weight Averaging (SWA)
- 通过对训练中的模型权重进行平均融合，提高模型鲁棒性。

## 7. 半监督技巧
- 使用pseudo label 或 meta pseudo label。

## 8. TTA (test time augmentation)

## 9. 数据增强
- 包括resize、crop、flip等CV操作，以及nlp的回译、词性替换等。

## 10. 蒸馏
- 参考Can Students Outperform Teachers in Knowledge Distillation论文。

## 11. 结构重参数化
- 参考RepVGG论文。

## 12. GradientCheckpoint
- 用于节省显存，提高建模自由度。

## 13. 终极策略
- 尝试更换random seed以观察不同的初始化效果。


